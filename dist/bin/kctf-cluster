#!/bin/bash
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
set -Eeuo pipefail

function kctf_cluster_start {
  MIN_NODES="1"
  MAX_NODES="2"
  NUM_NODES="1"
  MACHINE_TYPE="n2-standard-4"

  EXISTING_CLUSTER=$(gcloud container clusters list --filter "name=${CLUSTER_NAME}" --format 'get(name)')

  if [ -z "${EXISTING_CLUSTER}" ]; then
    gcloud container clusters create --release-channel=regular --enable-network-policy --enable-autoscaling --min-nodes ${MIN_NODES} --max-nodes ${MAX_NODES} --num-nodes ${NUM_NODES} --create-subnetwork name=kctf-${CLUSTER_NAME}-subnet --no-enable-master-authorized-networks --enable-ip-alias --enable-private-nodes --master-ipv4-cidr 172.16.0.32/28 --enable-autorepair --preemptible --machine-type ${MACHINE_TYPE} --workload-pool=${PROJECT}.svc.id.goog ${CLUSTER_NAME}
  fi

  EXISTING_ROUTER=$(gcloud compute routers list --filter "name=kctf-${CLUSTER_NAME}-nat-router" --format 'get(name)')
  if [ -z "${EXISTING_ROUTER}" ]; then
    gcloud compute routers create "kctf-${CLUSTER_NAME}-nat-router" --network=default --region "${ZONE::-2}"
  fi

  EXISTING_NAT=$(gcloud compute routers nats list --router "kctf-${CLUSTER_NAME}-nat-router" --router-region "${ZONE::-2}" --format 'get(name)')
  if [ -z "${EXISTING_NAT}" ]; then
    gcloud compute routers nats create "kctf-${CLUSTER_NAME}-nat-config" --router-region "${ZONE::-2}" --router kctf-${CLUSTER_NAME}-nat-router --nat-all-subnet-ip-ranges --auto-allocate-nat-external-ips
  fi

  kubectl create namespace "kctf-system" --dry-run=client -oyaml | kubectl apply -f - >&2

  # GCSFUSE

  SUFFIX=$(echo "${PROJECT}-${CLUSTER_NAME}-${ZONE}" | sha1sum)
  BUCKET_NAME="kctf-gcsfuse-${SUFFIX:0:16}"
  GCS_GSA_NAME="${BUCKET_NAME}"
  GCS_GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${GCS_GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)' || true)

  if [ -z "${GCS_GSA_EMAIL}" ]; then
    gcloud iam service-accounts create "${GCS_GSA_NAME}" --description "kCTF GCSFUSE service account ${CLUSTER_NAME} ${ZONE}" --display-name "kCTF GCSFUSE ${CLUSTER_NAME} ${ZONE}"
    GCS_GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${GCS_GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)')
    while [ -z "${GCS_GSA_EMAIL}" ]; do
      sleep 1
      GCS_GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${GCS_GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)')
    done
  fi

  # Creating CRD, rbac and operator
  kubectl apply -f "${KCTF_CTF_DIR}/kctf/resources/kctf.dev_challenges_crd.yaml"
  kubectl apply -f "${KCTF_CTF_DIR}/kctf/resources/rbac.yaml"
  kubectl apply -f "${KCTF_CTF_DIR}/kctf/resources/operator.yaml"

  OPERATOR_IMAGE=$("${KCTF_BIN}/yq" eval '.spec.template.spec.containers[0].image' "${KCTF_CTF_DIR}/kctf/resources/operator.yaml")

  # The operator needs to create some subresources, e.g. the gcsfuse service account
  for i in {1..30}; do
    kubectl get pods --namespace kctf-system -o=jsonpath="{.items[*].status.containerStatuses[?(@.image==\"${OPERATOR_IMAGE}\")].ready}" | grep "true" && break
    if [ "$i" == "30" ]; then
      echo "Couldn't find a kctf-operator pod with status ready=true and image=\"${OPERATOR_IMAGE}\" after 5 minutes" >&2
      kubectl get pods --namespace kctf-system -o=yaml >&2
      exit 1
    fi
    sleep 10
  done

  GCS_KSA_NAME="gcsfuse-sa"

  gcloud iam service-accounts add-iam-policy-binding --role roles/iam.workloadIdentityUser --member "serviceAccount:${PROJECT}.svc.id.goog[kctf-system/${GCS_KSA_NAME}]" ${GCS_GSA_EMAIL}
  kubectl annotate serviceaccount --namespace kctf-system ${GCS_KSA_NAME} iam.gke.io/gcp-service-account=${GCS_GSA_EMAIL} --overwrite

  if ! gsutil du "gs://${BUCKET_NAME}/"; then
    gsutil mb -l eu "gs://${BUCKET_NAME}/"
  fi

  if gsutil uniformbucketlevelaccess get "gs://${BUCKET_NAME}" | grep -q "Enabled: True"; then
    gsutil iam ch "serviceAccount:${GCS_GSA_EMAIL}:roles/storage.legacyBucketOwner" "gs://${BUCKET_NAME}"
    gsutil iam ch "serviceAccount:${GCS_GSA_EMAIL}:roles/storage.legacyObjectOwner" "gs://${BUCKET_NAME}"
  else
    gsutil acl ch -u "${GCS_GSA_EMAIL}:O" "gs://${BUCKET_NAME}"
  fi

  kubectl create configmap gcsfuse-config --from-literal=gcs_bucket="${BUCKET_NAME}" --namespace kctf-system --dry-run=client -o yaml | kubectl apply -f -

  kubectl patch ServiceAccount default --patch "automountServiceAccountToken: false"

  # Cloud DNS

  if [ ! -z "${DOMAIN_NAME}" ]
  then
    DNS_ZONE=$(gcloud dns managed-zones list --filter "dns_name:${DOMAIN_NAME}" --format 'get(name)')
    if [ -z "${DNS_ZONE}" ]; then
      gcloud dns managed-zones create "${CLUSTER_NAME}-dns-zone" --description "DNS Zone for ${DOMAIN_NAME}" --dns-name="${DOMAIN_NAME}."
    fi

    DNS_GSA_NAME="kctf-cloud-dns"
    DNS_GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${DNS_GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)' || true)

    if [ -z "${DNS_GSA_EMAIL}" ]; then
      gcloud iam service-accounts create "${DNS_GSA_NAME}" --description "kCTF Cloud DNS service account ${CLUSTER_NAME} ${ZONE}" --display-name "kCTF Cloud DNS ${CLUSTER_NAME} ${ZONE}"
      DNS_GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${DNS_GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)')
      while [ -z "${DNS_GSA_EMAIL}" ]; do
        sleep 1
        DNS_GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${DNS_GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)')
      done
    fi

    DNS_KSA_NAME="external-dns-sa"

    gcloud iam service-accounts add-iam-policy-binding --role roles/iam.workloadIdentityUser --member "serviceAccount:${PROJECT}.svc.id.goog[kctf-system/${DNS_KSA_NAME}]" ${DNS_GSA_EMAIL}
    kubectl annotate serviceaccount --namespace kctf-system ${DNS_KSA_NAME} iam.gke.io/gcp-service-account=${DNS_GSA_EMAIL} --overwrite

    gcloud projects add-iam-policy-binding ${PROJECT} --member=serviceAccount:${DNS_GSA_EMAIL} --role=roles/dns.admin

    kubectl create configmap --namespace kctf-system external-dns --from-literal=DOMAIN_NAME=${DOMAIN_NAME} --dry-run=client -o yaml | kubectl apply -f -
  fi
}

function kctf_cluster_stop {
  CLOUDSDK_CORE_DISABLE_PROMPTS=1 gcloud container clusters delete ${CLUSTER_NAME}
  gcloud compute routers delete "kctf-${CLUSTER_NAME}-nat-router" --region "${ZONE::-2}" --quiet

  SUFFIX=$(echo "${PROJECT}-${CLUSTER_NAME}-${ZONE}" | sha1sum)
  GSA_NAME="kctf-gcsfuse-${SUFFIX:0:16}"
  GSA_EMAIL=$(gcloud iam service-accounts list --filter "email=${GSA_NAME}@${PROJECT}.iam.gserviceaccount.com" --format 'get(email)' || true)
  if [ -z "${GSA_EMAIL}" ]; then
    gcloud iam service-accounts delete "${GSA_EMAIL}"
  fi
}

function kctf_cluster_resize_usage {
  echo -e "usage: kctf cluster resize [args]" >&2
  echo -e "args:" >&2
  echo -e "  -h|--help      print this help" >&2
  echo -e "  --machine-type machine type to use" >&2
  echo -e "                 to list available types, run: gcloud compute machine-types list --zones=\"${ZONE}\"" >&2
  echo -e "  --min-nodes    (required) minimum number of nodes in the cluster" >&2
  echo -e "  --max-nodes    (required) maximum number of nodes in the cluster" >&2
  echo -e "  --num-nodes    (required) initial number of nodes in the cluster" >&2
  echo -e "  --pool-name    name of the node pool" >&2
  echo -e "  --old-pool     name of the old pool to replace" >&2
}

function kctf_cluster_resize {
  OPTS="h"
  LONGOPTS="help,machine-type:,min-nodes:,max-nodes:,num-nodes:,pool-name:,old-pool"
  PARSED=$(getopt --options=$OPTS --longoptions=$LONGOPTS --name "kctf chal create" -- "$@")
  if [[ $? -ne 0 ]]; then
    kctf_cluster_resize_usage
    exit 1
  fi
  eval set -- "$PARSED"

  MACHINE_TYPE="n2-standard-4"
  MIN_NODES=
  MAX_NODES=
  NUM_NODES=
  NEW_POOL_NAME=
  OLD_POOL_NAME=
  while true; do
    case "$1" in
      -h|--help)
        kctf_cluster_resize_usage
        exit 0
        ;;
      --machine-type)
        MACHINE_TYPE="$2"
        shift 2
        ;;
      --min-nodes)
        MIN_NODES="$2"
        shift 2
        ;;
      --max-nodes)
        MAX_NODES="$2"
        shift 2
        ;;
      --num-nodes)
        NUM_NODES="$2"
        shift 2
        ;;
      --pool-name)
        NEW_POOL_NAME="$2"
        shift 2
        ;;
      --old-pool)
        OLD_POOL_NAME="$2"
        shift 2
        ;;
      --)
        shift
        break
        ;;
      *)
        echo "Unrecognized argument \"$1\"." >&2
        kctf_cluster_resize_usage
        exit 1
        ;;
    esac
  done

  if [[ -z "${MIN_NODES}" ]] || [[ -z "${MAX_NODES}" ]] || [[ -z "${NUM_NODES}" ]]; then
    echo "Required arguments missing" >&2
    kctf_cluster_resize_usage
    exit 1
  fi

  if [[ -z "${OLD_POOL_NAME}" ]]; then
    OLD_POOL_NAME=$(gcloud container node-pools list --cluster ${CLUSTER_NAME} --format 'value(name)')
    if [ $(echo "${OLD_POOL_NAME}" | wc -l) != "1" ]; then
      echo 'Multiple node pools found. Please choose which to replace with --old-pool.' >&2
      echo '== node pools ==' >&2
      echo "${OLD_POOL_NAME}" >&2
      exit 1
    fi
  fi

  if [[ -z "${NEW_POOL_NAME}" ]]; then
    NEW_POOL_NAME="${OLD_POOL_NAME}-resized"
  fi

  if [ "${OLD_POOL_NAME}" = "${NEW_POOL_NAME}" ]; then
    echo "New pool can't have the same name as the old pool." >&2
    exit 1
  fi

  if [[ $# -ne 0 ]]; then
    echo "Unrecognized arguments \"$@\"." >&2
    kctf_cluster_resize_usage
    exit 1
  fi

  echo 'Creating the new node pool'
  gcloud container node-pools create "${NEW_POOL_NAME}" \
    --cluster="${CLUSTER_NAME}" \
    --machine-type="${MACHINE_TYPE}" \
    --enable-autorepair \
    --enable-autoupgrade \
    --num-nodes="${NUM_NODES}" \
    --enable-autoscaling \
    --min-nodes="${MIN_NODES}" \
    --max-nodes="${MAX_NODES}"

  echo 'Cordoning old nodes'
  for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool="${OLD_POOL_NAME}" -o=name); do
    kubectl cordon "$node"
  done

  echo 'Draining old nodes'
  for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool="${OLD_POOL_NAME}" -o=name); do
    kubectl drain --force --ignore-daemonsets --delete-local-data --grace-period=10 "$node";
  done

  echo "Deleting old node pool \"${OLD_POOL_NAME}\""
  gcloud container node-pools delete "${OLD_POOL_NAME}" --cluster "${CLUSTER_NAME}"
}

function kctf_cluster_usage {
  echo -e "usage: kctf cluster command" >&2
  echo -e "available commands:" >&2
  echo -e "  start:  start the cluster" >&2
  echo -e "  stop:   stop the cluster" >&2
  echo -e "  resize: resize the cluster" >&2
}

if [[ $# -lt 1 ]]; then
  echo "unexpected argument count" >&2
  kctf_cluster_usage
  exit 1
fi

if [[ -z "${CLUSTER_NAME-}" ]]; then
  echo "No config loaded. You need to run \"kctf config\" first." >&2
  exit 1
fi

case "$1" in
  start)
    shift
    kctf_cluster_start $@
    ;;
  stop)
    shift
    kctf_cluster_stop $@
    ;;
  resize)
    shift
    kctf_cluster_resize $@
    ;;
  *)
    echo "unknown command" >&2
    kctf_cluster_usage
    exit 1
    ;;
esac

